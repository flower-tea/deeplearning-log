# 神经网络基础
## 总述
### 基本概念
- NN与人脑机理
- 分层：单层与多层，浅层与深层；端到端的计算
- 监督学习基本步骤
### 单层训练
- 学习规则：***Delta***、扩展Delta
- 更新算法：***SGD（随机梯度下降）***，批量算法与小批量算法
- 训练实例：基于matlab对一组数据的识别

## 学习
#### 神经元
计算方式：_v_ = _Wx_ + _b_                    _y = φ(v)_

其中 _φ_ 为激活函数

## 监督学习的基本步骤
1. 输出化权值系数
2. 提取训练样本，将其输入至NN中，计算结果与正确结果的误差 _e_
3. 根据公式，调整权值系数，减少上述误差
4. 重复23，直至遍历所有训练样本


## ***Delta规则***
> 单层NN代表性学习规则

```matlab
e = d - y                  %计算误差
W = W + alpha.*e*x;        %更改权值
```

### 梯度下降算法
> 最常用的一阶迭代最优化算法，用于求解某参数函数的***局部***最小值

```matlab
X = X - α .* df/dX
```

### ***一般化Delta规则***
更普遍的，使用Sigmoid激活函数，遍历每个样本，改变权值，训练n轮，称为SGD（随机梯度下降算法）
```matlab
v = W*x + b                    %计算结果
y = Sigmoid(v)                 %计算激活函数后的结果
e = d - y                      %计算误差
delta = y.*(1 - y).*e          %Sigmoid激活函数下的Delta规则
W = W + alpha.*delta*x;        %更改权值
```
[例题](https://github.com/flower-tea/deeplearning-log/edit/main/深度学习导论/课程代码/SGD)：根据训练数据 {0,0,1,0}{0,1,1,0}{1,0,1,1}{1,1,1,1}, 采用SGD算法，输出最后的结果




